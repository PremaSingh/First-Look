# -*- coding: utf-8 -*-
"""IWT_Resnet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18DZQcX11tayNbZUd4DKJsnnYgIQrUMNB
"""



"""Modify Dataset Class to Extract LL Band"""

import os
import pywt  # Wavelet Transform library
import numpy as np
from PIL import Image
import torch
from torch.utils.data import Dataset

class LungCancerDataset(Dataset):
    def __init__(self, root_dir, transform=None):
        self.root_dir = root_dir
        self.transform = transform
        self.image_paths = []
        self.labels = []

        # Class mapping
        self.class_mapping = {
            'Bengin cases': 0,
            'Malignant cases': 1,
            'Normal cases': 2
        }

        # Scan dataset folders
        for class_folder in os.listdir(root_dir):
            class_path = os.path.join(root_dir, class_folder)

            if not os.path.isdir(class_path):  # Skip non-folder items
                continue

            if class_folder not in self.class_mapping:  # Skip unknown folders
                print(f"Warning: Skipping unknown folder '{class_folder}'")
                continue

            label = self.class_mapping[class_folder]

            for img_name in os.listdir(class_path):
                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):  # Load only images
                    self.image_paths.append(os.path.join(class_path, img_name))
                    self.labels.append(label)

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        image = Image.open(self.image_paths[idx]).convert("L")  # Convert to grayscale
        image = np.array(image)  # Convert to numpy array

        # Apply Discrete Wavelet Transform (DWT) to get LL band
        LL, (LH, HL, HH) = pywt.dwt2(image, 'haar')

        # Convert LL to 3-channel by stacking
        LL_3channel = np.stack([LL, LL, LL], axis=-1)  # Shape: (H, W, 3)

        # Convert back to PIL Image
        image = Image.fromarray(np.uint8(LL_3channel))

        if self.transform:
            image = self.transform(image)

        label = self.labels[idx]
        return image, label

"""**Load Data Using LL Images**"""

from torchvision import transforms

# Define image transformations
transform = transforms.Compose([
    transforms.Resize((224, 224)),  # Resize image to match ResNet input size
    transforms.ToTensor(),  # Convert image to tensor
    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize pixel values
])

# Now use it in the dataset
dataset = LungCancerDataset(
    root_dir="/content/drive/My Drive/Lung Cancer Dataset/The IQ-OTHNCCD lung cancer dataset",
    transform=transform  # Now 'transform' is properly defined!
)

# Check dataset length
print(f"Total images in dataset: {len(dataset)}")

"""**Check if LL Images Are Loaded Correctly**"""

import matplotlib.pyplot as plt
import numpy as np
import torch

# Get a sample image from dataset
image, label = dataset[0]

# Convert tensor to NumPy
image_np = image.numpy()  # Convert to numpy
image_np = np.transpose(image_np, (1, 2, 0))  # Change shape from (3, 224, 224) -> (224, 224, 3)

# Show the image
plt.imshow(image_np, cmap="gray")  # Use cmap="gray" for LL band
plt.title(f"LL Band Image - Label: {label}")
plt.axis("off")
plt.show()

"""**Define ResNet Model**"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision import transforms
from torch.utils.data import DataLoader
class LungCancerResNet(nn.Module):
    def __init__(self, num_classes=3):  # 4 classes: Squamous, Large Cell, Adenocarcinoma, Normal
        super(LungCancerResNet, self).__init__()

        # Load Pretrained ResNet-18 Model
        self.resnet = models.resnet18(pretrained=True)

        # Modify input layer (if using grayscale images, change input channels to 1)
        self.resnet.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)

        # Modify the last fully connected layer for 4 classes
        num_ftrs = self.resnet.fc.in_features
        self.resnet.fc = nn.Linear(num_ftrs, num_classes)

    def forward(self, x):
        return self.resnet(x)

# Check model summary
model = LungCancerResNet(num_classes=3)
#print(model)
#

#pip install torchinfo

"""**Print Model Details**"""

import torch

# Set device (use GPU if available, otherwise use CPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

from torchinfo import summary

# Create the model
model = LungCancerResNet(num_classes=3).to(device)

# Print Model Summary
summary(model, input_size=(1, 3, 224, 224))  # (batch_size, channels, height, width)

"""**Define Training Parameters**"""

# Set device (GPU if available)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Move model to device
model = model.to(device)

# Define Loss Function (Cross-Entropy for Multi-Class Classification)
criterion = nn.CrossEntropyLoss()

# Define Optimizer (Adam)
optimizer = optim.Adam(model.parameters(), lr=0.0001)

# Learning Rate Scheduler
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)

"""**Train the Model**"""

def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=5):
    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        correct, total = 0, 0

        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)

            # Forward pass
            outputs = model(images)
            loss = criterion(outputs, labels)

            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            # Track loss & accuracy
            running_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        # Learning rate step
        scheduler.step()

        # Print training results
        train_acc = correct / total * 100
        print(f"Epoch [{epoch+1}/{num_epochs}] - Loss: {running_loss/len(train_loader):.4f} - Accuracy: {train_acc:.2f}%")

        # Validate the model
        validate_model(model, val_loader)

def validate_model(model, val_loader):
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    val_acc = correct / total * 100
    print(f"Validation Accuracy: {val_acc:.2f}%")

import os
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# Define transformations (resize, tensor conversion, normalize)
transform = transforms.Compose([
    transforms.Resize((224, 224)),  # Resize images for ResNet
    transforms.ToTensor(),  # Convert to tensor
    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize pixel values
])

# Define the correct dataset path (no 'train' folder)
dataset_path = "/content/drive/My Drive/Lung Cancer Dataset/The IQ-OTHNCCD lung cancer dataset"

# Load the full dataset
full_dataset = datasets.ImageFolder(root=dataset_path, transform=transform)

# Split dataset into train and validation sets
train_size = int(0.8 * len(full_dataset))  # 80% for training
val_size = len(full_dataset) - train_size  # 20% for validation
train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])

# Create DataLoaders
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

# Check if data is loaded correctly
print(f"Total training images: {len(train_dataset)}")
print(f"Total validation images: {len(val_dataset)}")
print(f"Classes: {full_dataset.classes}")

# Check class names
#print(f"Classes: {train_dataset.classes}")

# Check number of images
print(f"Total training images: {len(train_dataset)}")
print(f"Total validation images: {len(val_dataset)}")

# Show a sample image
import matplotlib.pyplot as plt
image, label = train_dataset[0]  # Get first image
plt.imshow(image.permute(1, 2, 0), cmap="gray")
plt.title(f"Class: {train_dataset.classes[label]}")
plt.show()

"""**Train the Model on LL Band Images**"""

# Confusion Matrix
import torch
import numpy as np
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt

# Define class labels
class_labels = ["Benign", "Malignant", "Normal", "Large Cell Carcinoma"]

def evaluate_model(model, val_loader):
    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, preds = torch.max(outputs, 1)

            all_preds.extend(preds.cpu().numpy())  # Store predictions
            all_labels.extend(labels.cpu().numpy())  # Store true labels

    # Compute confusion matrix
    cm = confusion_matrix(all_labels, all_preds)

    # Print Classification Report
    print("Classification Report:")
    print(classification_report(all_labels, all_preds, target_names=class_labels))

    # Plot Confusion Matrix
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=class_labels, yticklabels=class_labels)
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.title("Confusion Matrix")
    plt.show()

# Call the function after training
evaluate_model(model, val_loader)

train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=5)
